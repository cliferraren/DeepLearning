{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Smartphone Activity Detector\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Predict human activity using smartphone sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "Note: This dataset has already been scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Data Paths\n",
    "\n",
    "X_train_data = os.path.join('..','..','..', 'dataset','Train','X_train.txt')\n",
    "y_train_data = os.path.join('..','..','..', 'dataset','Train','y_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Data Paths\n",
    "\n",
    "X_testing_data = os.path.join('..','..','..', 'dataset','Test','X_test.txt')\n",
    "y_testing_data = os.path.join('..','..','..', 'dataset','Test','y_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>529</th>\n",
       "      <th>530</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039480</td>\n",
       "      <td>-0.002131</td>\n",
       "      <td>-0.029067</td>\n",
       "      <td>-0.998348</td>\n",
       "      <td>-0.982945</td>\n",
       "      <td>-0.971273</td>\n",
       "      <td>-0.998702</td>\n",
       "      <td>-0.983315</td>\n",
       "      <td>-0.974000</td>\n",
       "      <td>-0.802537</td>\n",
       "      <td>-0.736338</td>\n",
       "      <td>-0.712415</td>\n",
       "      <td>0.838758</td>\n",
       "      <td>0.708440</td>\n",
       "      <td>0.659340</td>\n",
       "      <td>-0.987427</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999826</td>\n",
       "      <td>-0.999411</td>\n",
       "      <td>-0.998918</td>\n",
       "      <td>-0.985482</td>\n",
       "      <td>-0.973481</td>\n",
       "      <td>-0.781973</td>\n",
       "      <td>-0.534604</td>\n",
       "      <td>-0.593165</td>\n",
       "      <td>0.607435</td>\n",
       "      <td>-0.266783</td>\n",
       "      <td>0.275882</td>\n",
       "      <td>0.200417</td>\n",
       "      <td>0.131266</td>\n",
       "      <td>-0.149017</td>\n",
       "      <td>0.292436</td>\n",
       "      <td>-0.192986</td>\n",
       "      <td>0.217496</td>\n",
       "      <td>-0.089175</td>\n",
       "      <td>0.059909</td>\n",
       "      <td>-0.236609</td>\n",
       "      <td>-0.012696</td>\n",
       "      <td>-0.072711</td>\n",
       "      <td>0.578649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999867</td>\n",
       "      <td>-0.991506</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.841270</td>\n",
       "      <td>0.533688</td>\n",
       "      <td>-0.625993</td>\n",
       "      <td>-0.898311</td>\n",
       "      <td>-0.988296</td>\n",
       "      <td>-0.983313</td>\n",
       "      <td>-0.982951</td>\n",
       "      <td>-0.987406</td>\n",
       "      <td>-0.992134</td>\n",
       "      <td>-0.988296</td>\n",
       "      <td>-0.999811</td>\n",
       "      <td>-0.993996</td>\n",
       "      <td>-0.720683</td>\n",
       "      <td>-0.948718</td>\n",
       "      <td>-0.268979</td>\n",
       "      <td>-0.364219</td>\n",
       "      <td>-0.723724</td>\n",
       "      <td>-0.995857</td>\n",
       "      <td>-0.996580</td>\n",
       "      <td>-0.995671</td>\n",
       "      <td>-0.996939</td>\n",
       "      <td>-0.994436</td>\n",
       "      <td>-0.995857</td>\n",
       "      <td>-0.999981</td>\n",
       "      <td>-0.994623</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.202804</td>\n",
       "      <td>-0.603199</td>\n",
       "      <td>-0.860677</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.845092</td>\n",
       "      <td>0.180261</td>\n",
       "      <td>-0.047436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.039978</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>-0.022651</td>\n",
       "      <td>-0.995482</td>\n",
       "      <td>-0.977314</td>\n",
       "      <td>-0.984760</td>\n",
       "      <td>-0.996415</td>\n",
       "      <td>-0.975835</td>\n",
       "      <td>-0.985973</td>\n",
       "      <td>-0.798477</td>\n",
       "      <td>-0.736338</td>\n",
       "      <td>-0.712415</td>\n",
       "      <td>0.834002</td>\n",
       "      <td>0.705008</td>\n",
       "      <td>0.674551</td>\n",
       "      <td>-0.988528</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.999719</td>\n",
       "      <td>-0.999803</td>\n",
       "      <td>-0.996898</td>\n",
       "      <td>-0.976781</td>\n",
       "      <td>-0.986754</td>\n",
       "      <td>-0.688176</td>\n",
       "      <td>-0.520514</td>\n",
       "      <td>-0.593165</td>\n",
       "      <td>0.272262</td>\n",
       "      <td>-0.056424</td>\n",
       "      <td>0.322283</td>\n",
       "      <td>-0.273292</td>\n",
       "      <td>0.037180</td>\n",
       "      <td>-0.133612</td>\n",
       "      <td>0.332487</td>\n",
       "      <td>-0.240491</td>\n",
       "      <td>0.348733</td>\n",
       "      <td>-0.195409</td>\n",
       "      <td>0.229436</td>\n",
       "      <td>-0.316816</td>\n",
       "      <td>-0.123889</td>\n",
       "      <td>-0.181137</td>\n",
       "      <td>0.608219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999845</td>\n",
       "      <td>-0.987029</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.904762</td>\n",
       "      <td>0.661975</td>\n",
       "      <td>-0.725887</td>\n",
       "      <td>-0.926663</td>\n",
       "      <td>-0.989255</td>\n",
       "      <td>-0.986019</td>\n",
       "      <td>-0.984533</td>\n",
       "      <td>-0.991701</td>\n",
       "      <td>-0.995857</td>\n",
       "      <td>-0.989255</td>\n",
       "      <td>-0.999854</td>\n",
       "      <td>-0.993256</td>\n",
       "      <td>-0.736521</td>\n",
       "      <td>-0.794872</td>\n",
       "      <td>-0.212429</td>\n",
       "      <td>-0.564868</td>\n",
       "      <td>-0.874594</td>\n",
       "      <td>-0.995034</td>\n",
       "      <td>-0.995308</td>\n",
       "      <td>-0.994868</td>\n",
       "      <td>-0.996133</td>\n",
       "      <td>-0.995863</td>\n",
       "      <td>-0.995034</td>\n",
       "      <td>-0.999973</td>\n",
       "      <td>-0.993834</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>0.440079</td>\n",
       "      <td>-0.404427</td>\n",
       "      <td>-0.761847</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.849230</td>\n",
       "      <td>0.180610</td>\n",
       "      <td>-0.042271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039785</td>\n",
       "      <td>-0.011809</td>\n",
       "      <td>-0.028916</td>\n",
       "      <td>-0.996194</td>\n",
       "      <td>-0.988569</td>\n",
       "      <td>-0.993256</td>\n",
       "      <td>-0.996994</td>\n",
       "      <td>-0.988526</td>\n",
       "      <td>-0.993135</td>\n",
       "      <td>-0.798477</td>\n",
       "      <td>-0.752778</td>\n",
       "      <td>-0.722186</td>\n",
       "      <td>0.834002</td>\n",
       "      <td>0.705008</td>\n",
       "      <td>0.673208</td>\n",
       "      <td>-0.990389</td>\n",
       "      <td>-0.999978</td>\n",
       "      <td>-0.999783</td>\n",
       "      <td>-0.999815</td>\n",
       "      <td>-0.996949</td>\n",
       "      <td>-0.989437</td>\n",
       "      <td>-0.992440</td>\n",
       "      <td>-0.715103</td>\n",
       "      <td>-0.860988</td>\n",
       "      <td>-0.916429</td>\n",
       "      <td>0.062816</td>\n",
       "      <td>0.082940</td>\n",
       "      <td>0.200566</td>\n",
       "      <td>-0.378262</td>\n",
       "      <td>0.090063</td>\n",
       "      <td>-0.209264</td>\n",
       "      <td>0.316530</td>\n",
       "      <td>-0.090862</td>\n",
       "      <td>0.396383</td>\n",
       "      <td>-0.353643</td>\n",
       "      <td>0.503754</td>\n",
       "      <td>-0.490389</td>\n",
       "      <td>-0.304759</td>\n",
       "      <td>-0.362708</td>\n",
       "      <td>0.506602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999894</td>\n",
       "      <td>-0.988427</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.680038</td>\n",
       "      <td>-0.702305</td>\n",
       "      <td>-0.907781</td>\n",
       "      <td>-0.989413</td>\n",
       "      <td>-0.987827</td>\n",
       "      <td>-0.987057</td>\n",
       "      <td>-0.987801</td>\n",
       "      <td>-0.996334</td>\n",
       "      <td>-0.989413</td>\n",
       "      <td>-0.999876</td>\n",
       "      <td>-0.989153</td>\n",
       "      <td>-0.720891</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.043398</td>\n",
       "      <td>-0.257142</td>\n",
       "      <td>-0.516341</td>\n",
       "      <td>-0.995224</td>\n",
       "      <td>-0.995417</td>\n",
       "      <td>-0.995951</td>\n",
       "      <td>-0.995346</td>\n",
       "      <td>-0.995728</td>\n",
       "      <td>-0.995224</td>\n",
       "      <td>-0.999974</td>\n",
       "      <td>-0.995305</td>\n",
       "      <td>-0.955696</td>\n",
       "      <td>-0.936508</td>\n",
       "      <td>0.430891</td>\n",
       "      <td>-0.138373</td>\n",
       "      <td>-0.491604</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848947</td>\n",
       "      <td>0.181907</td>\n",
       "      <td>-0.040826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038758</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.998241</td>\n",
       "      <td>-0.986774</td>\n",
       "      <td>-0.993115</td>\n",
       "      <td>-0.998216</td>\n",
       "      <td>-0.986479</td>\n",
       "      <td>-0.993825</td>\n",
       "      <td>-0.801982</td>\n",
       "      <td>-0.746505</td>\n",
       "      <td>-0.717858</td>\n",
       "      <td>0.838581</td>\n",
       "      <td>0.705854</td>\n",
       "      <td>0.673208</td>\n",
       "      <td>-0.995057</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.999882</td>\n",
       "      <td>-0.999908</td>\n",
       "      <td>-0.997772</td>\n",
       "      <td>-0.987726</td>\n",
       "      <td>-0.995109</td>\n",
       "      <td>-0.836774</td>\n",
       "      <td>-0.589200</td>\n",
       "      <td>-0.773771</td>\n",
       "      <td>0.312105</td>\n",
       "      <td>-0.095254</td>\n",
       "      <td>0.194399</td>\n",
       "      <td>-0.007998</td>\n",
       "      <td>0.266740</td>\n",
       "      <td>-0.318965</td>\n",
       "      <td>0.409731</td>\n",
       "      <td>-0.224589</td>\n",
       "      <td>0.520354</td>\n",
       "      <td>-0.319167</td>\n",
       "      <td>0.234376</td>\n",
       "      <td>-0.102650</td>\n",
       "      <td>-0.154974</td>\n",
       "      <td>-0.189796</td>\n",
       "      <td>0.598515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999941</td>\n",
       "      <td>-0.994542</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.560592</td>\n",
       "      <td>-0.529957</td>\n",
       "      <td>-0.857124</td>\n",
       "      <td>-0.991433</td>\n",
       "      <td>-0.989051</td>\n",
       "      <td>-0.987932</td>\n",
       "      <td>-0.992145</td>\n",
       "      <td>-0.998404</td>\n",
       "      <td>-0.991433</td>\n",
       "      <td>-0.999902</td>\n",
       "      <td>-0.989339</td>\n",
       "      <td>-0.763372</td>\n",
       "      <td>-0.897436</td>\n",
       "      <td>-0.270529</td>\n",
       "      <td>-0.539596</td>\n",
       "      <td>-0.833661</td>\n",
       "      <td>-0.995096</td>\n",
       "      <td>-0.995645</td>\n",
       "      <td>-0.995508</td>\n",
       "      <td>-0.995683</td>\n",
       "      <td>-0.997414</td>\n",
       "      <td>-0.995096</td>\n",
       "      <td>-0.999974</td>\n",
       "      <td>-0.995566</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.936508</td>\n",
       "      <td>0.137735</td>\n",
       "      <td>-0.366214</td>\n",
       "      <td>-0.702490</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.848164</td>\n",
       "      <td>0.185124</td>\n",
       "      <td>-0.037080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038988</td>\n",
       "      <td>0.004109</td>\n",
       "      <td>-0.017340</td>\n",
       "      <td>-0.997438</td>\n",
       "      <td>-0.993485</td>\n",
       "      <td>-0.996692</td>\n",
       "      <td>-0.997522</td>\n",
       "      <td>-0.993494</td>\n",
       "      <td>-0.996916</td>\n",
       "      <td>-0.801982</td>\n",
       "      <td>-0.743371</td>\n",
       "      <td>-0.716182</td>\n",
       "      <td>0.838581</td>\n",
       "      <td>0.718149</td>\n",
       "      <td>0.680539</td>\n",
       "      <td>-0.995964</td>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-0.999883</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.997352</td>\n",
       "      <td>-0.994801</td>\n",
       "      <td>-0.997161</td>\n",
       "      <td>-0.810251</td>\n",
       "      <td>-0.491561</td>\n",
       "      <td>-0.574107</td>\n",
       "      <td>0.388104</td>\n",
       "      <td>-0.139415</td>\n",
       "      <td>0.166019</td>\n",
       "      <td>0.097857</td>\n",
       "      <td>0.268970</td>\n",
       "      <td>-0.291559</td>\n",
       "      <td>0.398879</td>\n",
       "      <td>-0.319948</td>\n",
       "      <td>0.621990</td>\n",
       "      <td>-0.271919</td>\n",
       "      <td>0.221673</td>\n",
       "      <td>0.070146</td>\n",
       "      <td>-0.209111</td>\n",
       "      <td>-0.151092</td>\n",
       "      <td>0.179002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999937</td>\n",
       "      <td>-0.997070</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.249530</td>\n",
       "      <td>-0.521929</td>\n",
       "      <td>-0.800771</td>\n",
       "      <td>-0.990500</td>\n",
       "      <td>-0.985852</td>\n",
       "      <td>-0.985746</td>\n",
       "      <td>-0.989142</td>\n",
       "      <td>-0.994949</td>\n",
       "      <td>-0.990500</td>\n",
       "      <td>-0.999861</td>\n",
       "      <td>-0.991985</td>\n",
       "      <td>-0.768577</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.293201</td>\n",
       "      <td>-0.374032</td>\n",
       "      <td>-0.730931</td>\n",
       "      <td>-0.995147</td>\n",
       "      <td>-0.995419</td>\n",
       "      <td>-0.994627</td>\n",
       "      <td>-0.996161</td>\n",
       "      <td>-0.998516</td>\n",
       "      <td>-0.995147</td>\n",
       "      <td>-0.999974</td>\n",
       "      <td>-0.994610</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.074999</td>\n",
       "      <td>-0.554902</td>\n",
       "      <td>-0.844224</td>\n",
       "      <td>0.082632</td>\n",
       "      <td>-0.143439</td>\n",
       "      <td>0.275041</td>\n",
       "      <td>-0.368224</td>\n",
       "      <td>-0.849927</td>\n",
       "      <td>0.184795</td>\n",
       "      <td>-0.035326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2      ...          558       559       560\n",
       "0  0.039480 -0.002131 -0.029067    ...    -0.845092  0.180261 -0.047436\n",
       "1  0.039978 -0.005153 -0.022651    ...    -0.849230  0.180610 -0.042271\n",
       "2  0.039785 -0.011809 -0.028916    ...    -0.848947  0.181907 -0.040826\n",
       "3  0.038758 -0.002289 -0.023863    ...    -0.848164  0.185124 -0.037080\n",
       "4  0.038988  0.004109 -0.017340    ...    -0.849927  0.184795 -0.035326\n",
       "\n",
       "[5 rows x 561 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the training data into a data frame\n",
    "\n",
    "X_traindf = pd.read_csv(X_train_data, delimiter=\" \", skiprows=1, header=None)\n",
    "X_traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the dataframe to a numpy array for Keras\n",
    "X_train = X_traindf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the training labels as a dataframe\n",
    "y_traindf = pd.read_csv(y_train_data)\n",
    "\n",
    "# One-hot encode the integer labels\n",
    "# 1 WALKING\n",
    "# 2 WALKING_UPSTAIRS\n",
    "# 3 WALKING_DOWNSTAIRS\n",
    "# 4 SITTING\n",
    "# 5 STANDING\n",
    "# 6 LAYING\n",
    "# 7 STAND_TO_SIT\n",
    "# 8 SIT_TO_STAND\n",
    "# 9 SIT_TO_LIE\n",
    "# 10 LIE_TO_SIT\n",
    "# 11 STAND_TO_LIE\n",
    "# 12 LIE_TO_STAND\n",
    "\n",
    "y_train = to_categorical(y_traindf)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3161, 561)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the teting data\n",
    "X_testdf= pd.read_csv(X_testing_data, delimiter=\" \", skiprows=1, header=None)\n",
    "X_test = X_testdf.values\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3161, 13)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the testing labels\n",
    "y_testdf = pd.read_csv(y_testing_data)\n",
    "\n",
    "# One-hot encode the integer labels\n",
    "y_test = to_categorical(y_testdf)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the first layer where the input dimensions are the 561 columns of the training data\n",
    "model.add(Dense(100, activation='relu', input_dim= X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a second hidden layer\n",
    "model.add(Dense(100, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7766, 13)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The output layer has 13 columns that are one-hot encoded\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add output layer\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using categorical_crossentropy for the loss function, the adam optimizer,\n",
    "# and add accuracy to the training metrics\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 0.5396 - acc: 0.8018\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.2104 - acc: 0.9177\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.1567 - acc: 0.9430\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.1239 - acc: 0.9517\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.1037 - acc: 0.9601\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.1096 - acc: 0.9582\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.0866 - acc: 0.9654\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.0902 - acc: 0.9647\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.0697 - acc: 0.9724\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.0771 - acc: 0.9688\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.0783 - acc: 0.9677\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.0765 - acc: 0.9726\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.0587 - acc: 0.9785\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.0603 - acc: 0.9772\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.0693 - acc: 0.9741\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.0559 - acc: 0.9779\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.0632 - acc: 0.9731\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.0618 - acc: 0.9758\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.0471 - acc: 0.9838\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.0570 - acc: 0.9788\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.0516 - acc: 0.9812\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.0562 - acc: 0.9795\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.0508 - acc: 0.9797\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.0500 - acc: 0.9798\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.0490 - acc: 0.9826\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.0367 - acc: 0.9862\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.0502 - acc: 0.9800\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.0302 - acc: 0.9879\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.0335 - acc: 0.9875\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.0635 - acc: 0.9780\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.0438 - acc: 0.9831\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.0352 - acc: 0.9848\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.0312 - acc: 0.9873\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.0344 - acc: 0.9864\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.0380 - acc: 0.9856\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.0271 - acc: 0.9910\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.0243 - acc: 0.9910\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.0535 - acc: 0.9811\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.0348 - acc: 0.9862\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.0266 - acc: 0.9903\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.0216 - acc: 0.9918\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.0242 - acc: 0.9912\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.0407 - acc: 0.9843\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.0216 - acc: 0.9919\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.0285 - acc: 0.9896\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.0253 - acc: 0.9907\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.0149 - acc: 0.9954\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.0268 - acc: 0.9900\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.0145 - acc: 0.9947\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.0469 - acc: 0.9830\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.0150 - acc: 0.9943\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.0123 - acc: 0.9961\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.0140 - acc: 0.9943\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.0245 - acc: 0.9911\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.0257 - acc: 0.9907\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.0156 - acc: 0.9943\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.0109 - acc: 0.9963\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.0239 - acc: 0.9912\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.0319 - acc: 0.9893\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.0126 - acc: 0.9959\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.0169 - acc: 0.9933\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.0291 - acc: 0.9903\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.0340 - acc: 0.9892\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.0120 - acc: 0.9954\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.0148 - acc: 0.9941\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.0073 - acc: 0.9970\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.0210 - acc: 0.9933\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.0294 - acc: 0.9909\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.0117 - acc: 0.9946\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.0097 - acc: 0.9973\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.0166 - acc: 0.9936\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.0117 - acc: 0.9959\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.0098 - acc: 0.9974\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.0128 - acc: 0.9958\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.0094 - acc: 0.9961\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.0226 - acc: 0.9927\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.0181 - acc: 0.9927\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.0343 - acc: 0.9867\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.0100 - acc: 0.9967\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.0067 - acc: 0.9974\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.0082 - acc: 0.9972\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.0172 - acc: 0.9941\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.0089 - acc: 0.9968\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.0375 - acc: 0.9903\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.0238 - acc: 0.9928\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.0046 - acc: 0.9986\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.0118 - acc: 0.9963\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.0092 - acc: 0.9974\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.0166 - acc: 0.9954\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.0241 - acc: 0.9918\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.0323 - acc: 0.9887\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.0091 - acc: 0.9968\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.0065 - acc: 0.9977\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.0053 - acc: 0.9982\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.0106 - acc: 0.9967\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.0100 - acc: 0.9968\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.0088 - acc: 0.9968\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.0378 - acc: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd2ba8f1a58>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the training data to fit (train) the model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"../../../models/smartphone_trained.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from keras.models import load_model\n",
    "model = load_model(\"../../../models/smartphone_trained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3962929914538184, Accuracy: 0.932932616260677\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the training data\n",
    "model_loss, model_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 561)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab just one data point to test with\n",
    "test = np.expand_dims(X_test[0], axis=0)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [5]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction. The result should be 5 - STANDING\n",
    "print(f\"Predicted class: {model.predict_classes(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
